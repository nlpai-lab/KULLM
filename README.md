<p align="center" width="100%">
<img src="https://github.com/nlpai-lab/KULLM/raw/master/assets/logo.png" alt="NLP & AI Logo" style="width: 40%;">
<img src="https://i.imgur.com/NTlTxO3.png" alt="HIAI Logo" style="margin-left:5%; width: 40%;">
</p>

## Update Logs

- 2023.05.31:
  - [ğŸ¤—Polyglot-ko 12.8B ê¸°ë°˜ KULLM-Polyglot-12.8B-v2 fp16 ëª¨ë¸](https://huggingface.co/metterian/kullm-polyglot-12.8b) ê³µê°œ
  - êµ¬ë¦„(KULLM) ë°ì´í„°ì…‹ v2 ê³µê°œ
- 2023.05.30: [ğŸ¤—Polyglot-ko 12.8B ê¸°ë°˜ KULLM-Polyglot-12.8B fp16 ëª¨ë¸](https://huggingface.co/metterian/kullm-polyglot-12.8b) ê³µê°œ

---

<br>

# â˜ï¸ KULLM (êµ¬ë¦„): Korea University Large Langauge Model

KULLM(êµ¬ë¦„)ì€ ê³ ë ¤ëŒ€í•™êµ [NLP & AI ì—°êµ¬ì‹¤](http://blp.korea.ac.kr/)ê³¼ [HIAI ì—°êµ¬ì†Œ](http://hiai.korea.ac.kr)ì—ì„œ ê°œë°œí•œ, í•œêµ­ì–´ì— íŠ¹í™”ëœ LLM (Large Language Model) í”„ë¡œì íŠ¸ì…ë‹ˆë‹¤.

<br/>

## Example

<img src="assets/example.png" width="65%" >

<br/>

## í•œêµ­ì–´ ê¸°ë°˜ ëª¨ë¸(Polyglot-ko)

KULLM(êµ¬ë¦„)ì€ ë°±ë³¸ ëª¨ë¸ë¡œ í•œêµ­ì–´ ëª¨ë¸ì€ Polyglot-ko(12.8B)ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ í•™ìŠµì„ ì§„í–‰í–ˆìŠµë‹ˆë‹¤.

1. Polyglot-ko 12.8B ê¸°ë°˜-v2 -> ğŸ¤— [metterian/kullm-polyglot-12.8b-v2](https://huggingface.co/metterian/kullm-polyglot-12.8b-v2)
    - ë°ì´í„°ì…‹ v2: GPT4ALL, Dolly, Vicuna
2. Polyglot-ko 12.8B ê¸°ë°˜-v1 -> ğŸ¤— [metterian/kullm-polyglot-12.8b-v1](https://huggingface.co/metterian/kullm-polyglot-12.8b-v1)
    - ë°ì´í„°ì…‹ v1: GPT4ALL

Metaì˜ LLAMA ëª¨ë¸ê³¼ Polyglotì˜ 12.8B ì´í•˜ì˜ ëª¨ë¸ì€ í…ŒìŠ¤íŠ¸ ê²°ê³¼ í•œêµ­ì–´ ì„±ëŠ¥ì´ ì¢‹ì§€ ëª»í•˜ì—¬ ê³µê°œí•˜ì§€ ì•Šê¸°ë¡œ í–ˆìŠµë‹ˆë‹¤. ì¶”í›„ ì—¬ëŸ¬ ì¢‹ì€ í•œêµ­ì–´ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ëŠ” LLM ëª¨ë¸ì„ í•™ìŠµí•˜ì—¬ ê³µê°œí•˜ê³ ì í•©ë‹ˆë‹¤.

<br/>

## KoAlpaca ëª¨ë¸ ì‹¤í–‰ ì˜ˆì‹œ ì½”ë“œ

### Huggingface Pipelineìœ¼ë¡œ ì‹¤í–‰

- ìµœì‹ ë²„ì „ torch / HF ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜

```bash
pip install -U torch transformers tokenizers accelerate
```

ì•„ë˜ ì˜ˆì œ ì½”ë“œë¡œ ì‹¤í–‰í•´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
import torch
from transformers import pipeline, AutoModelForCausalLM

MODEL = 'metterian/kullm-polyglot-12.8b'

model = AutoModelForCausalLM.from_pretrained(
    MODEL,
    torch_dtype=torch.float16,
    low_cpu_mem_usage=True,
).to(device=f"cuda", non_blocking=True)
model.eval()

pipe = pipeline(
    'text-generation',
    model=model,
    tokenizer=MODEL,
    device=0
)

def ask(x, context='', is_input_full=False):
    ans = pipe(
        f"### ì§ˆë¬¸: {x}\n\n### ë§¥ë½: {context}\n\n### ë‹µë³€:" if context else f"### ì§ˆë¬¸: {x}\n\n### ë‹µë³€:",
        do_sample=True,
        max_new_tokens=512,
        temperature=0.7,
        top_p=0.9,
        return_full_text=False,
        eos_token_id=2,
    )
    print(ans[0]['generated_text'])

ask("ë”¥ëŸ¬ë‹ì´ ë­ì•¼?")
# ë”¥ëŸ¬ë‹ì€ ì¸ê³µì‹ ê²½ë§ì„ í†µí•´ ì…ë ¥ê³¼ ì¶œë ¥ ì‚¬ì´ì˜ ë³µì¡í•œ ê´€ê³„ë¥¼ í•™ìŠµí•˜ëŠ” ë¨¸ì‹ ëŸ¬ë‹ì˜ í•œ ë¶„ì•¼ì…ë‹ˆë‹¤. ì´ ê¸°ìˆ ì€ ì»´í“¨í„°ê°€ ì¸ê°„ì˜ í•™ìŠµ ëŠ¥ë ¥ê³¼ ìœ ì‚¬í•œ ë°©ì‹ìœ¼ë¡œ íŒ¨í„´ì„ í•™ìŠµí•˜ë„ë¡ í•˜ë©°, ì¸ê°„ì˜ ê°œì… ì—†ì´ë„ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤. ìµœê·¼ì—ëŠ” ë”¥ëŸ¬ë‹ì„ í™œìš©í•œ ì¸ê³µì§€ëŠ¥ ì• í”Œë¦¬ì¼€ì´ì…˜ì´ ë§ì´ ê°œë°œë˜ê³  ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì˜ë£Œ ì§„ë‹¨ ì• í”Œë¦¬ì¼€ì´ì…˜ì—ì„œëŠ” ë”¥ëŸ¬ë‹ ê¸°ìˆ ì„ í™œìš©í•˜ì—¬ í™˜ìì˜ íŠ¹ì§•ì„ íŒŒì•…í•˜ê³ , ì´ë¥¼ í†µí•´ ë¹ ë¥´ê³  ì •í™•í•œ ì§„ë‹¨ì„ ë‚´ë¦¬ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤. ë˜í•œ, ê¸ˆìœµ ë¶„ì•¼ì—ì„œëŠ” ë”¥ëŸ¬ë‹ ê¸°ìˆ ì„ í™œìš©í•˜ì—¬ ì£¼ê°€ ì˜ˆì¸¡ ëª¨í˜•ì„ í•™ìŠµí•˜ëŠ” ë° ì‚¬ìš©ë˜ê¸°ë„ í•©ë‹ˆë‹¤.
```

<br/>

## Dataset

ë°ì´í„°ì…‹ì€ ê¸°ë³¸ì ìœ¼ë¡œ [GPT4ALL](https://github.com/nomic-ai/gpt4all)ì„ ê¸°ë°˜ìœ¼ë¡œ í•©ë‹ˆë‹¤. GPT4ALLì€ ë›°ì–´ë‚œ instruction tuned assistant-style language modelë¡œ, ëˆ„êµ¬ë‚˜ ììœ ë¡­ê²Œ ì‚¬ìš©, ë°°í¬, í™•ì¥í•  ìˆ˜ ìˆë„ë¡ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤. GPT4ALL ë°ì´í„°ì…‹ì€ DEEPLì„ ì‚¬ìš©í•˜ì—¬ í•œêµ­ì–´ë¡œ ë²ˆì—­í•˜ì˜€ìŠµë‹ˆë‹¤.

#### ë°ì´í„°ì…‹ ì˜ˆì‹œ

GPT4ALL ë°ì´í„°ì…‹ì€ ë‹¤ìŒê³¼ ê°™ì´ Instruct ë¶€ë¶„ê³¼ Input, ê·¸ë¦¬ê³  Output ë¶€ë¶„ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ìˆìŠµë‹ˆë‹¤.

```json
...
{
    "id": "user_oriented_task_235",
    "motivation_app": "Yelp",
    "instruction": "ì „ë¬¸ ë¶„ì•¼ì— ë”°ë¼ ë ˆìŠ¤í† ë‘, í™ˆ ì„œë¹„ìŠ¤, ìë™ì°¨ ì„œë¹„ìŠ¤, ê¸°íƒ€ ì¤‘ í•˜ë‚˜ë¡œ ë¹„ì¦ˆë‹ˆìŠ¤ë¥¼ ë¶„ë¥˜í•©ë‹ˆë‹¤.",
    "instances": [
        {
            "input": "ê²¬ì ì„ ë°›ìœ¼ë ¤ë©´ 650-636-4884ë¡œ ì „í™”í•˜ê±°ë‚˜ ì›¹ì‚¬ì´íŠ¸ë¥¼ ë°©ë¬¸í•˜ì„¸ìš”. ì´ ë§¤ì¥ì€ ì‹ í’ˆ íƒ€ì´ì–´ ë° ì¼ë°˜ ìë™ì°¨ ìˆ˜ë¦¬ë¥¼ ì „ë¬¸ìœ¼ë¡œ í•©ë‹ˆë‹¤. ëª¨ë“  íƒ€ì´ì–´ë¥¼ ìì²´ì ìœ¼ë¡œ ë³´ìœ í•˜ê³  ìˆìœ¼ë©° ì˜ˆì‚°ì´ë‚˜ ì°¨ëŸ‰ íŠ¹ì„±ì— ë§ëŠ” ë‹¤ì–‘í•œ íƒ€ì´ì–´ë¥¼ ë³´ìœ í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì–´ë–¤ íƒ€ì´ì–´ê°€ í•„ìš”í•œì§€ ì˜ ëª¨ë¥´ì‹œê² ë‹¤ë©´ ì „ë¬¸ê°€ê°€ ìƒì£¼í•˜ì—¬ ê³ ê°ì˜ ìš”êµ¬ì— ê°€ì¥ ì í•©í•œ íƒ€ì´ì–´ë¥¼ ì„ íƒí•  ìˆ˜ ìˆë„ë¡ ë„ì™€ë“œë¦½ë‹ˆë‹¤. ë˜í•œ ìƒìš©ì°¨ íƒ€ì´ì–´ë„ ì·¨ê¸‰í•˜ê³  ìˆì–´ ë‹¤ì–‘í•œ ì°¨ëŸ‰ì— ë§ëŠ” íƒ€ì´ì–´ë¥¼ ì œê³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
            "output": "Auto Services"
        }
    ]
},
...
```

í•œêµ­ì–´ë¡œ ë²ˆì—­ëœ ë°ì´í„°ì…‹ì€ [`user_oriented_instructions_train.jsonl`](README.md
data/user_oriented_instructions_train.jsonl)ì— ì €ì¥ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

<br>

## Training with LoRA

KULLMì€ í•œêµ­ì–´ ëª¨ë¸ë¡œ Polyglot 12.8B ëª¨ë¸ì„ LoRA (Low Rank Adaptation)ë¥¼ ì‚¬ìš©í•˜ì—¬ í•™ìŠµí•˜ì˜€ìŠµë‹ˆë‹¤.

ëª¨ë¸ í•™ìŠµì€ A100 80GB 4ëŒ€ë¡œ ì§„í–‰í–ˆìŠµë‹ˆë‹¤. í•™ìŠµì— ì‚¬ìš©í•œ ì½”ë“œëŠ” [tloen/alpaca-lora](https://github.com/tloen/alpaca-lora)ì„ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš©í•˜ì˜€ìŠµë‹ˆë‹¤.

### KULLM v2

ğŸ¤— Huggingface Repo: [https://huggingface.co/metterian/kullm-polyglot-12.8b-v2](https://huggingface.co/metterian/kullm-polyglot-12.8b-v2)

ëª¨ë¸ í•™ìŠµì€ êµ¬ë¦„ ë°ì´í„°ì…‹ v2 (GPT4ALL, Dolly, Vicuna)ì„ ì‚¬ìš©í•˜ì—¬ ì§„í–‰í–ˆìŠµë‹ˆë‹¤. ì´ 8 epoch í•™ìŠµí•˜ì˜€ìœ¼ë©°, A100 80GB 4ëŒ€ë¥¼ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤.

### KULLM v1

ğŸ¤— Huggingface Repo: ğŸ¤— [https://huggingface.co/metterian/kullm-polyglot-12.8b-v1](https://huggingface.co/metterian/kullm-polyglot-12.8b-v1)

ëª¨ë¸ í•™ìŠµì€ êµ¬ë¦„ ë°ì´í„°ì…‹ v1 (GPT4ALL)ì„ ì‚¬ìš©í•˜ì—¬ ì§„í–‰í–ˆìŠµë‹ˆë‹¤. ì´ 5 epoch í•™ìŠµí•˜ì˜€ìœ¼ë©°, A100 80GB 4ëŒ€ë¥¼ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤.

### Dependency

1. ë‹¤ìŒ ëª…ë ¹ì–´ë¥¼ í†µí•´ í•„ìš”í•œ íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜:

```bash
pip install -r requirements.txt
```

2. ë§Œì•½ bitsandbytesê°€ ì‘ë™í•˜ì§€ ì•ŠëŠ”ë‹¤ë©´, [ì†ŒìŠ¤ì—ì„œ ì§ì ‘ ì„¤ì¹˜í•˜ì„¸ìš”](https://github.com/TimDettmers/bitsandbytes/blob/main/compile_from_source.md). ìœˆë„ìš° ì‚¬ìš©ìëŠ” [ë‹¤ìŒì˜ ì„¤ëª…ì„œ](https://github.com/tloen/alpaca-lora/issues/17)ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.

### Traning (`finetune_polyglot.py`)

ì´ ì½”ë“œëŠ” Polyglot ëª¨ë¸ì— PEFTë¥¼ ì§ì ‘ì ìœ¼ë¡œ ì ìš©í•˜ê³ , í”„ë¡¬í”„íŠ¸ êµ¬ì„± ë° í† í¬ë‚˜ì´ì§•ì— ê´€ë ¨ëœ ì½”ë“œê°€ ë“¤ì–´ìˆëŠ” íŒŒì¼ì…ë‹ˆë‹¤.

ì‚¬ìš© ì˜ˆì‹œ:

```
finetune_polyglot.py \
--base_model='EleutherAI/polyglot-ko-12.8b' \
--data_path='/data/persuade/01_KuAlpaca/alpaca_data_gpt4_deepl+gpt4_ko.jsonl'
```

ë‹¤ìŒê³¼ ê°™ì´ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì¡°ì •í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤:

```bash
python -m torch.distributed.launch  --master_port=34322  --nproc_per_node 4 finetune_polyglot.py \
    --fp16 \
    --base_model 'EleutherAI/polyglot-ko-12.8b' \
    --data_path data/user_oriented_instructions_train.jsonl \
    --output_dir ckpt/$SAVE_DIR \
    --batch_size 128 \
    --micro_batch_size 4 \
    --num_epochs $EPOCH \
    --learning_rate $LR \
    --cutoff_len 512 \
    --val_set_size 2000 \
    --lora_r 8 \
    --lora_alpha 16 \
    --lora_dropout 0.05 \
    --lora_target_modules '[q_proj,k_proj,v_proj,o_proj]' \
    --train_on_inputs \
    --logging_steps 1 \
    --eval_steps 40 \
    --weight_decay 0. \
    --warmup_steps 0 \
    --warmup_ratio 0.03 \
    --lr_scheduler_type "cosine" \
    --group_by_length
```

<br/>

## Evaluation

- ëª¨ë¸ í‰ê°€ëŠ” G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment (Yang Liu. et. al. 2023)ì˜ ë°©ë²•ë¡ ì„ ì‚¬ìš©í•˜ì˜€ìŠµë‹ˆë‹¤. í‰ê°€ ë°ì´í„°ì…‹ì€ [yizhongw/self-instruct](https://github.com/yizhongw/self-instruct)ì˜ íœ´ë¨¼ í‰ê°€ ë°ì´í„°ì…‹ì¸ `user_oriented_instructions.jsonl`ì„ deeplë¡œ ë²ˆì—­í•œ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ì˜€ìŠµë‹ˆë‹¤.

- í•´ë‹¹ ë°ì´í„°ì…‹ì€ [`user_oriented_instructions_eval.jsonl`](data/user_oriented_instructions_eval.jsonl)ì— ì €ì¥ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

#### Prompt

- TBA.

### LLM Inference Results for Korean Evaluation Set

| Type       | Model         | Score     | Releative Score (vs GPT4) |
| ---------- | ------------- | --------: | ------------------------: |
| Closed     | GPT4          | 87.6      | 100                       |
| Closed     | ChatGPT       | 83.3      | 95.1                      |
| Open       | **KULMM v2**  | **62.3**  | **71.1**                  |
| Open       | Alpaca v1.1   | 40.6      | 46.3                      |
| Open       | koVicuna      | 50.2      | 57.3                      |
---
