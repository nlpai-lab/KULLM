<p align="center" width="100%">
<img src="assets/logo.png" alt="NLP Logo" style="width: 90%;">
</p>

## Update Logs

- 2023.05.31:
  - ğŸ¤—Polyglot-ko 12.8B ê¸°ë°˜ [KULLM-Polyglot-12.8B-v2 ëª¨ë¸](https://huggingface.co/taeminlee/kullm-polyglot-12.8b-v2) ê³µê°œ
  - [êµ¬ë¦„(KULLM) ë°ì´í„°ì…‹ v2](http://gofile.me/6VWV1/PBpR0iYpq) ê³µê°œ
- 2023.05.30: ğŸ¤—Polyglot-ko 12.8B ê¸°ë°˜ [KULLM-Polyglot-12.8B ëª¨ë¸](https://huggingface.co/metterian/kullm-polyglot-12.8b) ê³µê°œ

---

<br>

# â˜ï¸ KULLM (êµ¬ë¦„): Korea University Large Langauge Model

KULLM(êµ¬ë¦„)ì€ ê³ ë ¤ëŒ€í•™êµ [NLP & AI ì—°êµ¬ì‹¤](http://blp.korea.ac.kr/)ê³¼ [HIAI ì—°êµ¬ì†Œ](http://hiai.korea.ac.kr)ì—ì„œ ê°œë°œí•œ, í•œêµ­ì–´ì— íŠ¹í™”ëœ LLM (Large Language Model) í”„ë¡œì íŠ¸ì…ë‹ˆë‹¤.

êµ¬ë¦„ í”„ë¡œì íŠ¸ëŠ” í•œêµ­ì–´ì— íŠ¹í™”ëœ ë°ì´í„°ì…‹ì„ ê³µê°œí•˜ì—¬ ë‹¤ì–‘í•œ íƒœìŠ¤í¬ë¥¼ ì•„ìš°ë¥´ëŠ” AI ëª¨ë¸ì„ ì œê³µí•˜ê³ ì í•©ë‹ˆë‹¤.

<br/>

## Example

<img src="assets/example.png" width="65%" >

<br/>

## í•œêµ­ì–´ ê¸°ë°˜ ëª¨ë¸(Polyglot-ko)

KULLM(êµ¬ë¦„)ì€ ë°±ë³¸ ëª¨ë¸ë¡œ í•œêµ­ì–´ ëª¨ë¸ì€ Polyglot-ko(12.8B)ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ í•™ìŠµì„ ì§„í–‰í–ˆìŠµë‹ˆë‹¤.

1. Polyglot-ko 12.8B ê¸°ë°˜-v2 -> ğŸ¤— [taeminlee/kullm-polyglot-12.8b-v2](https://huggingface.co/taeminlee/kullm-polyglot-12.8b-v2)
    - ë°ì´í„°ì…‹ v2: [GPT4ALL](https://github.com/nomic-ai/gpt4all), [Dolly](https://github.com/databrickslabs/dolly), [Vicuna](https://github.com/lm-sys/FastChat)
2. Polyglot-ko 12.8B ê¸°ë°˜-v1 -> ğŸ¤— [metterian/kullm-polyglot-12.8b-v1](https://huggingface.co/metterian/kullm-polyglot-12.8b-v1)
    - ë°ì´í„°ì…‹ v1: GPT4ALL

Metaì˜ LLAMA ëª¨ë¸ê³¼ Polyglotì˜ 12.8B ì´í•˜ì˜ ëª¨ë¸ì€ í…ŒìŠ¤íŠ¸ ê²°ê³¼ í•œêµ­ì–´ ì„±ëŠ¥ì´ ì¢‹ì§€ ëª»í•˜ì—¬ ê³µê°œí•˜ì§€ ì•Šê¸°ë¡œ í–ˆìŠµë‹ˆë‹¤. ì¶”í›„ ì—¬ëŸ¬ ì¢‹ì€ í•œêµ­ì–´ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ëŠ” LLM ëª¨ë¸ì„ í•™ìŠµí•˜ì—¬ ê³µê°œí•˜ê³ ì í•©ë‹ˆë‹¤.

<br/>

## KULLM ëª¨ë¸ ì‹¤í–‰ ì˜ˆì‹œ ì½”ë“œ

### Huggingface Pipelineìœ¼ë¡œ ì‹¤í–‰

- ìµœì‹ ë²„ì „ torch / HF ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜

```bash
pip install -U torch transformers tokenizers accelerate
```

ì•„ë˜ ì˜ˆì œ ì½”ë“œë¡œ ì‹¤í–‰í•´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline

from utils.prompter import Prompter

MODEL = "taeminlee/kullm-polyglot-12.8b-v2"

model = AutoModelForCausalLM.from_pretrained(
    MODEL,
    torch_dtype=torch.float16,
    low_cpu_mem_usage=True,
).to(device=f"cuda", non_blocking=True)
model.eval()

pipe = pipeline("text-generation", model=model, tokenizer=MODEL, device=0)

prompter = Prompter("kullm")


def infer(instruction="", input_text=""):
    prompt = prompter.generate_prompt(instruction, input_text)
    output = pipe(prompt, max_length=512, temperature=0.2, num_beams=5, eos_token_id=2)
    s = output[0]["generated_text"]
    result = prompter.get_response(s)

    return result


result = infer(input_text="ê³ ë ¤ëŒ€í•™êµì— ëŒ€í•´ì„œ ì•Œë ¤ì¤˜")
print(result)
# 'ê³ ë ¤ëŒ€í•™êµì— ëŒ€í•´ ê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ì‹œë©´ ì–¸ì œë“ ì§€ ë¬¸ì˜í•´ ì£¼ì„¸ìš”. ê³ ë ¤ëŒ€í•™êµëŠ” í•œêµ­ì—ì„œ ê°€ì¥ ì˜¤ë˜ë˜ê³  ê¶Œìœ„ ìˆëŠ” ëŒ€í•™êµ ì¤‘ í•˜ë‚˜ë¡œ, ê³ ë ¤ëŒ€í•™êµì˜ ì—­ì‚¬ëŠ” í•œêµ­ì˜ ì—­ì‚¬ì™€ í•¨ê»˜í•´ ì™”ìŠµë‹ˆë‹¤. ê³ ë ¤ëŒ€í•™êµëŠ” í•™ë¬¸ì  ìš°ìˆ˜ì„±ì„ ì¶”êµ¬í•˜ëŠ” ë™ì‹œì— ì‚¬íšŒì  ì±…ì„ì„ ë‹¤í•˜ê¸° ìœ„í•´ ìµœì„ ì„ ë‹¤í•˜ê³  ìˆìŠµë‹ˆë‹¤. ê³ ë ¤ëŒ€í•™êµëŠ” í•™ìƒ, êµìˆ˜ì§„, êµì§ì›ì„ ìœ„í•œ ë‹¤ì–‘í•œ í”„ë¡œê·¸ë¨ê³¼ ì§€ì›ì„ ì œê³µí•˜ëŠ” ê²ƒìœ¼ë¡œ ìœ ëª…í•©ë‹ˆë‹¤. ê³ ë ¤ëŒ€í•™êµëŠ” í•œêµ­ì˜ ì •ì¹˜, ê²½ì œ, ì‚¬íšŒ ë¶„ì•¼ì—ì„œ ì¤‘ìš”í•œ ì—­í• ì„ ë‹´ë‹¹í•˜ê³  ìˆìŠµë‹ˆë‹¤. ê³ ë ¤ëŒ€í•™êµì— ëŒ€í•´ ë” ìì„¸íˆ ì•Œê³  ì‹¶ìœ¼ì‹ ê°€ìš”?'
```

<br/>

## Dataset

### êµ¬ë¦„ ë°ì´í„°ì…‹ v2

[ë‹¤ìš´ë¡œë“œ](http://gofile.me/6VWV1/PBpR0iYpq)

êµ¬ë¦„ ë°ì´í„°ì…‹ v2ëŠ” [GPT4ALL](https://github.com/nomic-ai/gpt4all), [Vicuna](https://github.com/lm-sys/FastChat), ê·¸ë¦¬ê³  Databricksì˜ [Dolly](https://github.com/databrickslabs/dolly) ë°ì´í„°ì…‹ì„ ë³‘í•©í•œ ê²ƒì…ë‹ˆë‹¤. ì´ ëª¨ë“  ë°ì´í„°ì…‹ì€ DeepLì„ ì´ìš©í•˜ì—¬ í•œêµ­ì–´ë¡œ ë²ˆì—­ë˜ì—ˆìŠµë‹ˆë‹¤.

GPT4ALLì€ instruction tuned assistant-style language modelì´ë©°, Vicunaì™€ Dolly ë°ì´í„°ì…‹ì€ ë‹¤ì–‘í•œ ìì—°ì–´ ì²˜ë¦¬ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ë° í™œìš©ë©ë‹ˆë‹¤. íŠ¹íˆ, DollyëŠ” instruction/response fine tuning recordsë¥¼ í›ˆë ¨ ë°ì´í„°ë¡œ ì‚¬ìš©í•œ ì–¸ì–´ ëª¨ë¸ì…ë‹ˆë‹¤.

ë‹¤ìš´ë¡œë“œ í›„, `data` í´ë”ë¡œ ì´ë™í•´ì£¼ì„¸ìš”.

### êµ¬ë¦„ ë°ì´í„°ì…‹ v1

[êµ¬ë¦„ ë°ì´í„°ì…‹ v1](./data/kullm-v1.jsonl)ì€ GPT4ALLì„ ê¸°ë°˜ìœ¼ë¡œ í•©ë‹ˆë‹¤.

#### ë°ì´í„°ì…‹ ì˜ˆì‹œ

GPT4ALL ë°ì´í„°ì…‹ì€ ë‹¤ìŒê³¼ ê°™ì´ Instruct ë¶€ë¶„ê³¼ Input, ê·¸ë¦¬ê³  Output ë¶€ë¶„ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ìˆìŠµë‹ˆë‹¤.

```json
{
    "id": "user_oriented_task_235",
    "motivation_app": "Yelp",
    "instruction": "ì „ë¬¸ ë¶„ì•¼ì— ë”°ë¼ ë ˆìŠ¤í† ë‘, í™ˆ ì„œë¹„ìŠ¤, ìë™ì°¨ ì„œë¹„ìŠ¤, ê¸°íƒ€ ì¤‘ í•˜ë‚˜ë¡œ ë¹„ì¦ˆë‹ˆìŠ¤ë¥¼ ë¶„ë¥˜í•©ë‹ˆë‹¤.",
    "instances": [
        {
            "input": "ê²¬ì ì„ ë°›ìœ¼ë ¤ë©´ 650-636-4884ë¡œ ì „í™”í•˜ê±°ë‚˜ ì›¹ì‚¬ì´íŠ¸ë¥¼ ë°©ë¬¸í•˜ì„¸ìš”. ì´ ë§¤ì¥ì€ ì‹ í’ˆ íƒ€ì´ì–´ ë° ì¼ë°˜ ìë™ì°¨ ìˆ˜ë¦¬ë¥¼ ì „ë¬¸ìœ¼ë¡œ í•©ë‹ˆë‹¤. ëª¨ë“  íƒ€ì´ì–´ë¥¼ ìì²´ì ìœ¼ë¡œ ë³´ìœ í•˜ê³  ìˆìœ¼ë©° ì˜ˆì‚°ì´ë‚˜ ì°¨ëŸ‰ íŠ¹ì„±ì— ë§ëŠ” ë‹¤ì–‘í•œ íƒ€ì´ì–´ë¥¼ ë³´ìœ í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì–´ë–¤ íƒ€ì´ì–´ê°€ í•„ìš”í•œì§€ ì˜ ëª¨ë¥´ì‹œê² ë‹¤ë©´ ì „ë¬¸ê°€ê°€ ìƒì£¼í•˜ì—¬ ê³ ê°ì˜ ìš”êµ¬ì— ê°€ì¥ ì í•©í•œ íƒ€ì´ì–´ë¥¼ ì„ íƒí•  ìˆ˜ ìˆë„ë¡ ë„ì™€ë“œë¦½ë‹ˆë‹¤. ë˜í•œ ìƒìš©ì°¨ íƒ€ì´ì–´ë„ ì·¨ê¸‰í•˜ê³  ìˆì–´ ë‹¤ì–‘í•œ ì°¨ëŸ‰ì— ë§ëŠ” íƒ€ì´ì–´ë¥¼ ì œê³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
            "output": "Auto Services"
        }
    ]
},
```

í•œêµ­ì–´ë¡œ ë²ˆì—­ëœ ë°ì´í„°ì…‹ì€ [`kullm-v2.jsonl`](data/kullm-v2.jsonl)ì— ì €ì¥ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

<br>

## Training with LoRA

KULLMì€ í•œêµ­ì–´ ëª¨ë¸ë¡œ Polyglot 12.8B ëª¨ë¸ì„ LoRA (Low Rank Adaptation)ë¥¼ ì‚¬ìš©í•˜ì—¬ í•™ìŠµí•˜ì˜€ìŠµë‹ˆë‹¤.

ëª¨ë¸ í•™ìŠµì€ A100 80GB 4ëŒ€ë¡œ ì§„í–‰í–ˆìŠµë‹ˆë‹¤. í•™ìŠµì— ì‚¬ìš©í•œ ì½”ë“œëŠ” [tloen/alpaca-lora](https://github.com/tloen/alpaca-lora)ì„ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš©í•˜ì˜€ìŠµë‹ˆë‹¤.

### KULLM v2

ğŸ¤— Huggingface Repo: [https://huggingface.co/taeminlee/kullm-polyglot-12.8b-v2](https://huggingface.co/taeminlee/kullm-polyglot-12.8b-v2)

ëª¨ë¸ í•™ìŠµì€ êµ¬ë¦„ ë°ì´í„°ì…‹ v2 (GPT4ALL, Dolly, Vicuna)ì„ ì‚¬ìš©í•˜ì—¬ ì§„í–‰í–ˆìŠµë‹ˆë‹¤. ì´ 8 epoch í•™ìŠµí•˜ì˜€ìœ¼ë©°, A100 80GB 4ëŒ€ë¥¼ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤.

### KULLM v1

ğŸ¤— Huggingface Repo: ğŸ¤— [https://huggingface.co/metterian/kullm-polyglot-12.8b-v1](https://huggingface.co/metterian/kullm-polyglot-12.8b-v1)

ëª¨ë¸ í•™ìŠµì€ êµ¬ë¦„ ë°ì´í„°ì…‹ v1 (GPT4ALL)ì„ ì‚¬ìš©í•˜ì—¬ ì§„í–‰í–ˆìŠµë‹ˆë‹¤. ì´ 5 epoch í•™ìŠµí•˜ì˜€ìœ¼ë©°, A100 80GB 4ëŒ€ë¥¼ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤.

### Dependency

1. ë‹¤ìŒ ëª…ë ¹ì–´ë¥¼ í†µí•´ í•„ìš”í•œ íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜:

```bash
pip install -r requirements.txt
```

2. ë§Œì•½ bitsandbytesê°€ ì‘ë™í•˜ì§€ ì•ŠëŠ”ë‹¤ë©´, [ì†ŒìŠ¤ì—ì„œ ì§ì ‘ ì„¤ì¹˜í•˜ì„¸ìš”](https://github.com/TimDettmers/bitsandbytes/blob/main/compile_from_source.md). ìœˆë„ìš° ì‚¬ìš©ìëŠ” [ë‹¤ìŒì˜ ì„¤ëª…ì„œ](https://github.com/tloen/alpaca-lora/issues/17)ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.

### Traning (`finetune_polyglot.py`)

ì´ ì½”ë“œëŠ” Polyglot ëª¨ë¸ì— PEFTë¥¼ ì§ì ‘ì ìœ¼ë¡œ ì ìš©í•˜ê³ , í”„ë¡¬í”„íŠ¸ êµ¬ì„± ë° í† í¬ë‚˜ì´ì§•ì— ê´€ë ¨ëœ ì½”ë“œê°€ ë“¤ì–´ìˆëŠ” íŒŒì¼ì…ë‹ˆë‹¤.

ì‚¬ìš© ì˜ˆì‹œ:

```
python finetune_polyglot.py \
--base_model='EleutherAI/polyglot-ko-12.8b' \
--data_path='./data/kullm-v2.jsonl'
```

ë‹¤ìŒê³¼ ê°™ì´ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì¡°ì •í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤:

```bash
python -m torch.distributed.launch  --master_port=34322  --nproc_per_node 4 finetune_polyglot.py \
--fp16 \
--base_model 'EleutherAI/polyglot-ko-12.8b' \
--data_path data/kullm-v2.jsonl \
--output_dir ckpt/$SAVE_DIR \
--prompt_template_name kullm \
--batch_size 128 \
--micro_batch_size 4 \
--num_epochs $EPOCH \
--learning_rate $LR \
--cutoff_len 512 \
--val_set_size 2000 \
--lora_r 8 \
--lora_alpha 16 \
--lora_dropout 0.05 \
--lora_target_modules "[query_key_value, xxx]" \
--train_on_inputs \
--logging_steps 1 \
--eval_steps 40 \
--weight_decay 0. \
--warmup_steps 0 \
--warmup_ratio 0.1 \
--lr_scheduler_type "cosine" \
--group_by_length
```

<br/>

## Evaluation

- ëª¨ë¸ í‰ê°€ëŠ” G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment (Yang Liu. et. al. 2023)ì˜ ë°©ë²•ë¡ ì„ ì‚¬ìš©í•˜ì˜€ìŠµë‹ˆë‹¤. í‰ê°€ ë°ì´í„°ì…‹ì€ [yizhongw/self-instruct](https://github.com/yizhongw/self-instruct)ì˜ íœ´ë¨¼ í‰ê°€ ë°ì´í„°ì…‹ì¸ `user_oriented_instructions.jsonl`ì„ deeplë¡œ ë²ˆì—­í•œ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ì˜€ìŠµë‹ˆë‹¤.

- í•´ë‹¹ ë°ì´í„°ì…‹ì€ [`user_oriented_instructions_eval.jsonl`](data/user_oriented_instructions_eval.jsonl)ì— ì €ì¥ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

#### Prompt

- TBA.

### LLM Inference Results for Korean Evaluation Set

| Type   | Base-model        | Model                                                        | Score    | Releative Score (vs GPT4) |
| ------ | ----------------- | ------------------------------------------------------------ | -------- | ------------------------- |
| Closed | GPT4              | GPT4                                                         | 87.6     | 100                       |
| Closed | GPT3.5-turbo      | GPT3.5-turbo                                                 | 83.3     | 95.1                      |
| Open   | Polyglot-ko-12.8b | **KULMM v2**                                                 | **62.3** | **71.1**                  |
| Open   | Polyglot-ko-5.8b  | [KoAlpaca v1.1](https://huggingface.co/beomi/KoAlpaca-Polyglot-5.8B) | 40.6     | 46.3                      |
| Open   | LLaMA-7b          | [koVicuna](https://huggingface.co/junelee/ko_vicuna_7b)      | 50.2     | 57.3                      |

---